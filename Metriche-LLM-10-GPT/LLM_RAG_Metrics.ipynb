{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/gbarbaro/UniQA/lora_ft_reduced_chat/it_test.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trova_indice_domanda(domanda):\n",
    "    for i, item in enumerate(data):\n",
    "        if item['question'] == domanda:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "domanda_input = \"\"\n",
    "indice = trova_indice_domanda(domanda_input)\n",
    "\n",
    "if indice != -1:\n",
    "    print(f\"L'indice della domanda Ã¨: {indice}\")\n",
    "else:\n",
    "    print(\"Domanda non trovata nel file JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [0, 1, 1156, 1158, 1162, 1168, 1170, 1184, 1238, 1241]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = list(range(1249))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import Faithfulness, AnswerCorrectness\n",
    "from ragas import evaluate\n",
    "import nest_asyncio\n",
    "import os\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "    \n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"token\"\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset \n",
    "\n",
    "# Carica i dataset di valutazione\n",
    "with open('/home/gbarbaro/Test/Inferenza/output_metric_total/output_metric_total_ANITA.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "j=0\n",
    "output = [data[j]['output_pairs']['output'][i] for i in index] \n",
    "reference = [data[j]['output_pairs']['reference'][i] for i in index]\n",
    "\n",
    "with open('/home/gbarbaro/UniQA/lora_ft_reduced_chat/it_test.json', 'r') as file:\n",
    "    data_input = json.load(file)\n",
    "\n",
    "user_input = [data_input[i][\"question\"] for i in index]\n",
    "\n",
    "retrieved_contexts = []\n",
    "\n",
    "for i in index:\n",
    "    if \"Documents\" in data_input[i][\"input\"]: \n",
    "        indice = data_input[i][\"input\"].find(\"Documents\") \n",
    "        retrieved_contexts.append(data_input[i][\"input\"][indice + len(\"Documents:\\n\"):].strip().split('\\n')) \n",
    "    else: retrieved_contexts.append([])\n",
    "\n",
    "# Converti output_pairs in un formato compatibile con Hugging Face Dataset \n",
    "dataset_dict = {\"user_input\": user_input, \"reference\": reference, \"response\": output, \"retrieved_contexts\": retrieved_contexts} \n",
    "hf_dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def evaluate_metrics(references, predictions):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "    rouge = evaluate.load('rouge')\n",
    "    rouge_score = rouge.compute(predictions=predictions,references=references)\n",
    "    \n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"it\", device=\"cuda\")\n",
    "    \n",
    "    meteor = evaluate.load('meteor')\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    return bleu_score, rouge_score, bert_score, meteor_score\n",
    "\n",
    "def mean(list):\n",
    "    mean = sum(list)/len(list)\n",
    "    \n",
    "    return mean\n",
    "    \n",
    "output_pairs = {'output': hf_dataset[\"response\"], 'reference': hf_dataset[\"reference\"]}\n",
    "\n",
    "bleu_score, rouge_score, bert_score, meteor_score = evaluate_metrics(output_pairs['reference'], output_pairs['output'])\n",
    "    \n",
    "print(f\"BLEU score: {round(bleu_score['bleu'], 2)}\")\n",
    "print(f\"ROUGE score: {rouge_score}\")\n",
    "print(f\"BERT_PRECISION score: {round(mean(bert_score['precision']), 2)}\")\n",
    "print(f\"BERT_RECALL score: {round(mean(bert_score['recall']), 2)}\")\n",
    "print(f\"BERT_F1 score: {round(mean(bert_score['f1']), 2)}\")\n",
    "print(f\"METEOR score: {round(meteor_score['meteor'], 2)}\")\n",
    "\n",
    "output_metric = {'output_pairs': output_pairs,\n",
    "                'bleu_score': bleu_score,\n",
    "                'rouge_score': rouge_score,\n",
    "                'bert_score': bert_score,\n",
    "                'meteor_score': meteor_score,\n",
    "                'perplexity_score': 0,\n",
    "                'perplexity_z_score': 0\n",
    "                }\n",
    "    \n",
    "try:\n",
    "    with open('output_metric.json', 'r') as file:\n",
    "        output_metric_json = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    output_metric_json = []\n",
    "except json.JSONDecodeError:\n",
    "    output_metric_json = []\n",
    "\n",
    "output_metric_json.append(output_metric)\n",
    "    \n",
    "with open('output_metric_10.json', 'w') as file:\n",
    "    json.dump(output_metric_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    AnswerCorrectness(llm = evaluator_llm),\n",
    "    Faithfulness(llm = evaluator_llm),\n",
    "]\n",
    "\n",
    "results = evaluate(dataset=hf_dataset, metrics=metrics, raise_exceptions=False)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voto Soggettivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/gbarbaro/Test/Metriche-LLM-Z-GPT/output_total.json', 'r') as file:\n",
    "    data_r = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/gbarbaro/Test/Inferenza_Fine_Tuning/Risultati_FT/output_metric_LLAMA_3.3_FT.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [0, 1, 1156, 1158, 1162, 1168, 1170, 1184, 1238, 1241]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0]['output_pairs']['output'][1241])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_r['reference'][1241])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
